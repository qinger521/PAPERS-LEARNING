Paper | Authors |  Venue |  Link
-|-|-|-
Learning Deep Transformer Models for Machine Translation | Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, Lidia S. Chao | EMNLP-2019 |https://arxiv.org/pdf/1906.01787.pdf|
Improving Deep Transformer with Depth-Scaled Initialization and Merged Attention | Biao Zhang, Ivan Titov, Rico Sennrich | EMNLP-2019 |https://www.aclweb.org/anthology/D19-1083.pdf|
Lipschitz Constrained Parameter Initialization for Deep Transformers | Hongfei Xu, Qiuhui Liu ,Josef van Genabith,Deyi Xiong,Jingyi Zhang | ACL-2020 |https://www.aclweb.org/anthology/2020.acl-main.38.pdf|
Multiscale Collaborative Deep Models for Neural Machine Translation | Xiangpeng Wei, Heng Yu, Yue Hu, Yue Zhang, Rongxiang Weng, Weihua Luo | ACL-2020 |https://www.aclweb.org/anthology/2020.acl-main.40.pdf|
Highway Transformer: Self-Gating Enhanced Self-Attentive Networks | Yekun Chai, Shuo Jin, Xinwen Hou | ACL-2020 |https://arxiv.org/pdf/2004.08178.pdf|
Training Deeper Neural Machine Translation Models with Transparent Attention| Ankur Bapna ,Mia Xu Chen , Orhan Firat , Yuan Cao ,Yonghui Wu | ACL-2018 |https://arxiv.org/pdf/1808.07561v1.pdf|
UNIVERSAL TRANSFORMERS| Mostafa Dehghani | ICLR 2019 |https://arxiv.org/pdf/1807.03819.pdf|
Deep Residual Learning for Image Recognition| Kaiming He,Xiangyu Zhang | CVPR-2016 |https://arxiv.org/pdf/1512.03385.pdf|
Layer-Wise Coordination between Encoder and Decoder for Neural Machine Translation| Tianyu He | NIPS-2018 |https://proceedings.neurips.cc/paper/2018/file/4fb8a7a22a82c80f2c26fe6c1e0dcbb3-Paper.pdf|
Improving Transformer Optimization Through Better Initialization| Xiao Shi Huang  | ICML-2020 |http://www.cs.toronto.edu/~mvolkovs/ICML2020_tfixup.pdf|
Understanding the Difficulty of Training Transformers| Liyuan Liu  | arXiv |https://arxiv.org/pdf/2004.08249.pdf|